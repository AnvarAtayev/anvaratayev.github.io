[
  {
    "id": "KellyJr.1956",
    "author": [
      {
        "family": "Kelly Jr.",
        "given": "J. L."
      }
    ],
    "citation-key": "KellyJr.1956",
    "container-title": "Bell System Technical Journal",
    "DOI": "10.1002/j.1538-7305.1956.tb03809.x",
    "ISSN": "1538-7305",
    "issue": "4",
    "issued": {
      "date-parts": [
        [
          "1956"
        ]
      ]
    },
    "language": "en",
    "license": "© 1956 The Bell System Technical Journal",
    "page": "917–926",
    "source": "Wiley Online Library",
    "title": "A New Interpretation of Information Rate",
    "type": "article-journal",
    "URL": "https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1956.tb03809.x",
    "volume": "35"
  },
  {
    "id": "Ferguson2009",
    "author": [
      {
        "family": "Ferguson",
        "given": "Thomas"
      }
    ],
    "citation-key": "Ferguson2009",
    "collection-title": "UCLA",
    "container-title": "Game Theory for Statisticians",
    "issued": {
      "date-parts": [
        [
          "2009"
        ]
      ]
    },
    "language": "en",
    "source": "Zotero",
    "title": "The Kelly Betting System for Favorable Games.",
    "type": "article-journal",
    "URL": "https://www.math.ucla.edu/~tom/stat596.html"
  },
  {
    "id": "Thorp2006",
    "author": [
      {
        "family": "Thorp",
        "given": "Edward O."
      }
    ],
    "citation-key": "Thorp2006",
    "container-title": "Handbook of Asset and Liability Management",
    "DOI": "10.1016/S1872-0978(06)01009-X",
    "ISBN": "978-0-444-50875-1",
    "issued": {
      "date-parts": [
        [
          "2006"
        ]
      ]
    },
    "language": "en",
    "page": "385–428",
    "publisher": "Elsevier",
    "source": "DOI.org (Crossref)",
    "title": "Chapter 9 The Kelly Criterion in Blackjack Sports Betting, and the Stock Market",
    "type": "chapter",
    "URL": "https://linkinghub.elsevier.com/retrieve/pii/S187209780601009X",
    "volume": "1"
  },
  {
    "id": "VonNeumann2007",
    "author": [
      {
        "family": "Von Neumann",
        "given": "John"
      },
      {
        "family": "Morgenstern",
        "given": "Oskar"
      }
    ],
    "citation-key": "VonNeumann2007",
    "collection-title": "A Princeton classic edition",
    "edition": "60. anniversary ed., 4. print., and 1. paperb. print",
    "event-place": "Princeton, NJ",
    "ISBN": "978-0-691-13061-3 978-1-4008-2946-0",
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    },
    "language": "eng",
    "number-of-pages": "739",
    "publisher": "Princeton University Press",
    "publisher-place": "Princeton, NJ",
    "source": "K10plus ISBN",
    "title": "Theory of games and economic behavior",
    "type": "book"
  },
  {
    "id": "MacLean2010",
    "author": [
      {
        "family": "MacLean",
        "given": "Leonard C"
      },
      {
        "family": "Chair",
        "given": "Herbert Lamb"
      },
      {
        "family": "Thorp",
        "given": "Edward O"
      },
      {
        "family": "Beach",
        "given": "Newport"
      },
      {
        "family": "Ziemba",
        "given": "William T"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "citation-key": "MacLean2010",
    "language": "en",
    "source": "Zotero",
    "title": "Good and bad properties of the Kelly criterion",
    "type": "article-journal"
  },
  {
    "id": "Conway",
    "author": [
      {
        "family": "Wikipedia contributors"
      }
    ],
    "title": "Conway's Game of Life - Wikipedia, The Free Encyclopedia",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "URL": "https://en.wikipedia.org/w/index.php?title=Conway%27s_Game_of_Life&oldid=1323115836"
  },
  {
    "id": "Gappy2025",
    "abstract": "We were born to suffer",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Gappy",
        "given": ""
      }
    ],
    "citation-key": "Gappy2025",
    "container-title": "At night we walk in circles and are consumed by fire",
    "genre": "Substack newsletter",
    "issued": {
      "date-parts": [
        [
          "2025",
          11,
          20
        ]
      ]
    },
    "title": "Reading, Writing and Drawdown Arithmetic",
    "type": "post-weblog",
    "URL": "https://byfire.substack.com/p/reading-writing-and-drawdown-arithmetic?utm_medium=ios"
  },
  {
    "id": "Zhang2012",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Zhang",
        "given": "Hongzhong"
      },
      {
        "family": "Hadjiliadis",
        "given": "Olympia"
      }
    ],
    "citation-key": "Zhang2012",
    "container-title": "Methodology and Computing in Applied Probability",
    "container-title-short": "Methodol Comput Appl Probab",
    "DOI": "10.1007/s11009-011-9262-7",
    "ISSN": "1387-5841, 1573-7713",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          "2012",
          9
        ]
      ]
    },
    "language": "en",
    "license": "http://www.springer.com/tdm",
    "page": "739–752",
    "source": "DOI.org (Crossref)",
    "title": "Drawdowns and the Speed of Market Crash",
    "type": "article-journal",
    "URL": "http://link.springer.com/10.1007/s11009-011-9262-7",
    "volume": "14"
  },
  {
    "id": "Salminen2025",
    "abstract": "In this paper, using the excursion theory, we provide new proofs of, firstly, Lehoczky’s formula (in an extended form allowing a lower bound for the underlying diffusion) for the joint distribution of the first drawdown time and the maximum before this time, and, secondly, of Malyutin’s formula for the joint distribution of the first hitting time and the maximum drawdown before this time. It is remarkable – but there is a clean explanation – that the excursion theoretical approach which we developed first for Lehoczky’s formula also provides a proof for Malyutin’s formula. Moreover, we discuss some generalizations and analyze the pure jump process describing the maximum before the first drawdown time when the size of the drawdown is varying.",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          22
        ]
      ]
    },
    "author": [
      {
        "family": "Salminen",
        "given": "Paavo"
      },
      {
        "family": "Vallois",
        "given": "Pierre"
      }
    ],
    "citation-key": "Salminen2025",
    "container-title": "ESAIM: Probability and Statistics",
    "container-title-short": "ESAIM: PS",
    "DOI": "10.1051/ps/2025010",
    "ISSN": "1262-3318",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "license": "https://creativecommons.org/licenses/by/4.0",
    "page": "357–380",
    "source": "DOI.org (Crossref)",
    "title": "Drawdowns of Diffusions",
    "type": "article-journal",
    "URL": "https://www.esaim-ps.org/10.1051/ps/2025010",
    "volume": "29"
  },
  {
    "id": "Taylor1975",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          23
        ]
      ]
    },
    "author": [
      {
        "family": "Taylor",
        "given": "Howard M."
      }
    ],
    "citation-key": "Taylor1975",
    "container-title": "The Annals of Probability",
    "container-title-short": "Ann. Probab.",
    "DOI": "10.1214/aop/1176996395",
    "ISSN": "0091-1798",
    "issue": "2",
    "issued": {
      "date-parts": [
        [
          "1975",
          4,
          1
        ]
      ]
    },
    "source": "DOI.org (Crossref)",
    "title": "A Stopped Brownian Motion Formula",
    "type": "article-journal",
    "URL": "https://projecteuclid.org/journals/annals-of-probability/volume-3/issue-2/A-Stopped-Brownian-Motion-Formula/10.1214/aop/1176996395.full",
    "volume": "3"
  },
  {
    "id": "Borodin2002",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Borodin",
        "given": "Andrei N."
      },
      {
        "family": "Salminen",
        "given": "Paavo"
      }
    ],
    "citation-key": "Borodin2002",
    "collection-title": "Probability and Its Applications",
    "DOI": "10.1007/978-3-0348-8163-0",
    "event-place": "Basel",
    "ISBN": "978-3-7643-6705-3 978-3-0348-8163-0",
    "issued": {
      "date-parts": [
        [
          "2002"
        ]
      ]
    },
    "license": "http://www.springer.com/tdm",
    "publisher": "Birkhäuser",
    "publisher-place": "Basel",
    "source": "DOI.org (Crossref)",
    "title": "Handbook of Brownian Motion - Facts and Formulae",
    "type": "book",
    "URL": "http://link.springer.com/10.1007/978-3-0348-8163-0"
  },
  {
    "id": "Shreve2008",
    "author": [
      {
        "family": "Shreve",
        "given": "Steven"
      }
    ],
    "citation-key": "Shreve2008",
    "collection-title": "Springer finance textbook",
    "edition": "Corr. print",
    "event-place": "New York, NY",
    "ISBN": "978-0-387-40101-0",
    "issued": {
      "date-parts": [
        [
          "2008"
        ]
      ]
    },
    "language": "eng",
    "number-of-pages": "550",
    "publisher": "Springer",
    "publisher-place": "New York, NY",
    "source": "K10plus ISBN",
    "title": "Stochastic calculus for finance. 2: Continuous-time models",
    "title-short": "Stochastic calculus for finance. 2",
    "type": "chapter"
  },
  {
    "id": "WikiExp2025",
    "abstract": "In probability theory and statistics, the exponential distribution or negative exponential distribution is the probability distribution of the distance between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate; the distance parameter could be any meaningful mono-dimensional measure of the process, such as time between production errors, or length along a roll of fabric in the weaving manufacturing process. It is a particular case of the gamma distribution. It is the continuous analogue of the geometric distribution, and it has the key property of being memoryless. In addition to being used for the analysis of Poisson point processes it is found in various other contexts.\nThe exponential distribution is not the same as the class of exponential families of distributions. This is a large class of probability distributions that includes the exponential distribution as one of its members, but also includes many other distributions, like the normal, binomial, gamma, and Poisson distributions.",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Wikipedia Contributors",
        "given": ""
      }
    ],
    "citation-key": "WikiExp2025",
    "container-title": "Wikipedia",
    "issued": {
      "date-parts": [
        [
          "2025",
          11,
          6
        ]
      ]
    },
    "language": "en",
    "license": "Creative Commons Attribution-ShareAlike License",
    "note": "Page Version ID: 1320816285",
    "source": "Wikipedia",
    "title": "Exponential distribution",
    "type": "entry-encyclopedia",
    "URL": "https://en.wikipedia.org/w/index.php?title=Exponential_distribution&oldid=1320816285"
  },
  {
    "id": "WikiIG2025",
    "abstract": "In probability theory, the inverse Gaussian distribution (also known as the Wald distribution) is a two-parameter family of continuous probability distributions with support on (0,∞).\nIts probability density function is given by\n\n  \n    \n      \n        f\n        (\n        x\n        ;\n        μ\n        ,\n        λ\n        )\n        =\n        \n          \n            \n              λ\n              \n                2\n                π\n                \n                  x\n                  \n                    3\n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          \n            (\n          \n        \n        −\n        \n          \n            \n              λ\n              (\n              x\n              −\n              μ\n              \n                )\n                \n                  2\n                \n              \n            \n            \n              2\n              \n                μ\n                \n                  2\n                \n              \n              x\n            \n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle f(x;\\mu ,\\lambda )={\\sqrt {\\frac {\\lambda }{2\\pi x^{3}}}}\\exp {\\biggl (}-{\\frac {\\lambda (x-\\mu )^{2}}{2\\mu ^{2}x}}{\\biggr )}}\n  \n\nfor x > 0, where \n  \n    \n      \n        μ\n        >\n        0\n      \n    \n    {\\displaystyle \\mu >0}\n  \n is the mean and \n  \n    \n      \n        λ\n        >\n        0\n      \n    \n    {\\displaystyle \\lambda >0}\n  \n is the shape parameter.\nThe inverse Gaussian distribution has several properties analogous to a Gaussian distribution.  The name can be misleading:  it is an inverse only in that, while the Gaussian describes a Brownian motion's level at a fixed time, the inverse Gaussian describes the distribution of the time a Brownian motion with positive drift takes to reach a fixed positive level.\nIts cumulant generating function (logarithm of the characteristic function) is the inverse of the cumulant generating function of a Gaussian random variable.\nTo indicate that a random variable X is inverse Gaussian-distributed with mean μ and shape parameter λ we write \n  \n    \n      \n        X\n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        λ\n        )\n        \n        \n      \n    \n    {\\displaystyle X\\sim \\operatorname {IG} (\\mu ,\\lambda )\\,\\!}\n  \n.",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          24
        ]
      ]
    },
    "author": [
      {
        "family": "Wikipedia Contributors",
        "given": ""
      }
    ],
    "citation-key": "WikiIG2025",
    "container-title": "Wikipedia",
    "issued": {
      "date-parts": [
        [
          "2025",
          5,
          25
        ]
      ]
    },
    "language": "en",
    "license": "Creative Commons Attribution-ShareAlike License",
    "note": "Page Version ID: 1292168347",
    "source": "Wikipedia",
    "title": "Inverse Gaussian distribution",
    "type": "entry-encyclopedia",
    "URL": "https://en.wikipedia.org/w/index.php?title=Inverse_Gaussian_distribution&oldid=1292168347"
  },
  {
    "id": "Ruggles1947",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          27
        ]
      ]
    },
    "author": [
      {
        "family": "Ruggles",
        "given": "Richard"
      },
      {
        "family": "Brodie",
        "given": "Henry"
      }
    ],
    "citation-key": "Ruggles1947",
    "container-title": "Journal of the American Statistical Association",
    "container-title-short": "Journal of the American Statistical Association",
    "DOI": "10.1080/01621459.1947.10501915",
    "ISSN": "0162-1459, 1537-274X",
    "issue": "237",
    "issued": {
      "date-parts": [
        [
          "1947",
          3
        ]
      ]
    },
    "language": "en",
    "page": "72–91",
    "source": "DOI.org (Crossref)",
    "title": "An Empirical Approach to Economic Intelligence in World War II",
    "type": "article-journal",
    "URL": "http://www.tandfonline.com/doi/abs/10.1080/01621459.1947.10501915",
    "volume": "42"
  },
  {
    "id": "Goodman1952",
    "abstract": "The problem discussed is that of sampling without replacement from a discrete, finite, uniform population. One source of this problem is the analysis of serial numbers on manufactured items in order to estimate the total number of items manufactured. Minimum variance unbiased estimators of the parameters are obtained and compared with other estimators which have been suggested. Tests of hypothesis and confidence intervals are also discussed.",
    "accessed": {
      "date-parts": [
        [
          "2025",
          11,
          27
        ]
      ]
    },
    "author": [
      {
        "family": "Goodman",
        "given": "Leo A."
      }
    ],
    "citation-key": "Goodman1952",
    "container-title": "Journal of the American Statistical Association",
    "DOI": "10.1080/01621459.1952.10483442",
    "ISSN": "0162-1459",
    "issue": "260",
    "issued": {
      "date-parts": [
        [
          "1952",
          12,
          1
        ]
      ]
    },
    "page": "622–634",
    "publisher": "ASA Website",
    "source": "Taylor and Francis+NEJM",
    "title": "Serial Number Analysis",
    "type": "article-journal",
    "URL": "https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483442",
    "volume": "47"
  },
  {
    "id": "Casella2002",
    "author": [
      {
        "family": "Casella",
        "given": "George"
      },
      {
        "family": "Berger",
        "given": "Roger L."
      }
    ],
    "citation-key": "Casella2002",
    "edition": "2. ed",
    "event-place": "Pacific Grove, Calif",
    "ISBN": "978-0-534-24312-8",
    "issued": {
      "date-parts": [
        [
          "2002"
        ]
      ]
    },
    "language": "eng",
    "number-of-pages": "660",
    "publisher": "Duxbury",
    "publisher-place": "Pacific Grove, Calif",
    "source": "K10plus ISBN",
    "title": "Statistical Inference",
    "type": "book"
  },
  {
    "id": "BBCInfiniteMonkey2025",
    "author": [
      {
        "family": "BBC",
        "given": "Radio 4"
      }
    ],
    "citation-key": "BBCInfiniteMonkey2025",
    "container-title": "BBC",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "language": "en",
    "source": "BBC",
    "title": "The Infinite Monkey Cage: 2025 Christmas Special",
    "type": "webpage",
    "URL": "https://www.bbc.co.uk/programmes/m002kjvk"
  },
  {
    "id": "NOAAWanderingPoles",
    "author": [
      {
        "family": "NOAA",
        "given": "NCEI"
      }
    ],
    "citation-key": "NOAAWanderingPoles",
    "container-title": "NOAA National Centers for Environmental Information",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "language": "en",
    "source": "NOAA NCEI",
    "title": "Wandering Geomagnetic Poles",
    "type": "webpage",
    "URL": "https://www.ncei.noaa.gov/products/wandering-geomagnetic-poles"
  },
  {
    "id": "WikiOymyakon",
    "author": [
      {
        "family": "Wikipedia contributors"
      }
    ],
    "citation-key": "WikiOymyakon",
    "container-title": "Wikipedia",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "language": "en",
    "source": "Wikipedia",
    "title": "Oymyakon",
    "type": "webpage",
    "URL": "https://en.wikipedia.org/wiki/Oymyakon"
  },
  {
    "id": "WikiInaccessibility",
    "author": [
      {
        "family": "Wikipedia contributors"
      }
    ],
    "citation-key": "WikiInaccessibility",
    "container-title": "Wikipedia",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "language": "en",
    "source": "Wikipedia",
    "title": "Pole of inaccessibility",
    "type": "webpage",
    "URL": "https://en.wikipedia.org/wiki/Pole_of_inaccessibility"
  },
  {
    "id": "WikiCelestialPole",
    "author": [
      {
        "family": "Wikipedia contributors"
      }
    ],
    "citation-key": "WikiCelestialPole",
    "container-title": "Wikipedia",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "language": "en",
    "source": "Wikipedia",
    "title": "Celestial pole",
    "type": "webpage",
    "URL": "https://en.wikipedia.org/wiki/Celestial_pole"
  },
  {
    "id": "WikiGeomagneticPole",
    "author": [
      {
        "family": "Wikipedia contributors"
      }
    ],
    "citation-key": "WikiGeomagneticPole",
    "container-title": "Wikipedia",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "language": "en",
    "source": "Wikipedia",
    "title": "Geomagnetic pole",
    "type": "webpage",
    "URL": "https://en.wikipedia.org/wiki/Geomagnetic_pole"
  },
  {
    "id": "WikiMagneticPole",
    "author": [
      {
        "family": "Wikipedia contributors"
      }
    ],
    "citation-key": "WikiMagneticPole",
    "container-title": "Wikipedia",
    "issued": {
      "date-parts": [
        [
          "2025"
        ]
      ]
    },
    "language": "en",
    "source": "Wikipedia",
    "title": "North magnetic pole",
    "type": "webpage",
    "URL": "https://en.wikipedia.org/wiki/North_magnetic_pole"
  },
  {
    "id": "Downey2024",
    "abstract": "The inspection paradox is a statistical illusion you’ve probably never heard of. But once you learn about it, you see it everywhere.",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          9
        ]
      ]
    },
    "author": [
      {
        "family": "Downey",
        "given": "Allen"
      }
    ],
    "citation-key": "Downey2024",
    "container-title": "TDS Archive",
    "issued": {
      "date-parts": [
        [
          "2024"
        ]
      ]
    },
    "language": "en",
    "title": "The Inspection Paradox is Everywhere",
    "type": "post-weblog",
    "URL": "https://medium.com/data-science/the-inspection-paradox-is-everywhere-2ef1c2e9d709"
  },
  {
    "id": "VanderPlas2018",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          9
        ]
      ]
    },
    "author": [
      {
        "family": "VanderPlas",
        "given": "Jake"
      }
    ],
    "citation-key": "VanderPlas2018",
    "container-title": "Pythonic Perambulations",
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "title": "The Waiting Time Paradox, or, Why Is My Bus Always Late?",
    "type": "post-weblog",
    "URL": "https://jakevdp.github.io/blog/2018/09/13/waiting-time-paradox/"
  },
  {
    "id": "Grimmett2020",
    "abstract": "The fourth edition of this successful text provides an introduction to probability and random processes, with many practical applications. It is aimed at mathematics undergraduates and postgraduates, and has four main aims.  \n                \n                 To provide a thorough but straightforward account of basic probability theory, giving the reader a natural feel for the subject unburdened by oppressive technicalities.  \n                 To discuss important random processes in depth with many examples. \n                 To cover a range of topics that are significant and interesting but less routine.  \n                 To impart to the beginner some flavour of advanced work. \n                \n                \n                The book begins with the basic ideas common to most undergraduate courses in mathematics, statistics, and science. It ends with material usually found at graduate level, for example, Markov processes, (including Markov chain Monte Carlo), martingales, queues, diffusions, (including stochastic calculus with Itô's formula), renewals, stationary processes (including the ergodic theorem), and option pricing in mathematical finance using the Black-Scholes formula. Further, in this new revised fourth edition, there are sections on coupling from the past, Lévy processes, self-similarity and stability, time changes, and the holding-time/jump-chain construction of continuous-time Markov chains. Finally, the number of exercises and problems has been increased by around 300 to a total of about 1300, and many of the existing exercises have been refreshed by additional parts. The solutions to these exercises and problems can be found in the companion volume, One Thousand Exercises in Probability, third edition, (OUP 2020).\n               \n               \n                \n              ,  \n               The fourth edition of this successful text provides an introduction to probability and random processes, with many practical applications. It is aimed at mathematics undergraduates and postgraduates, and has four main aims.  \n                \n                 To provide a thorough but straightforward account of basic probability theory, giving the reader a natural feel for the subject unburdened by oppressive technicalities.  \n                 To discuss important random processes in depth with many examples. \n                 To cover a range of topics that are significant and interesting but less routine.  \n                 To impart to the beginner some flavour of advanced work. \n                \n                \n                The book begins with the basic ideas common to most undergraduate courses in mathematics, statistics, and science. It ends with material usually found at graduate level, for example, Markov processes, (including Markov chain Monte Carlo), martingales, queues, diffusions, (including stochastic calculus with Itô's formula), renewals, stationary processes (including the ergodic theorem), and option pricing in mathematical finance using the Black-Scholes formula. Further, in this new revised fourth edition, there are sections on coupling from the past, Lévy processes, self-similarity and stability, time changes, and the holding-time/jump-chain construction of continuous-time Markov chains. Finally, the number of exercises and problems has been increased by around 300 to a total of about 1300, and many of the existing exercises have been refreshed by additional parts. The solutions to these exercises and problems can be found in the companion volume, One Thousand Exercises in Probability, third edition, (OUP 2020).",
    "author": [
      {
        "family": "Grimmett",
        "given": "Geoffrey"
      },
      {
        "family": "Stirzaker",
        "given": "David"
      },
      {
        "family": "Grimmett",
        "given": "Geoffrey"
      },
      {
        "family": "Stirzaker",
        "given": "David"
      }
    ],
    "citation-key": "Grimmett2020",
    "ISBN": "978-0-19-884759-5",
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "number-of-pages": "688",
    "publisher": "Oxford University Press",
    "publisher-place": "Oxford, New York",
    "source": "Oxford University Press",
    "title": "Probability and Random Processes",
    "type": "book"
  },
  {
    "id": "Ruder2017",
    "abstract": "Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          17
        ]
      ]
    },
    "author": [
      {
        "family": "Ruder",
        "given": "Sebastian"
      }
    ],
    "citation-key": "Ruder2017",
    "DOI": "10.48550/arXiv.1609.04747",
    "issued": {
      "date-parts": [
        [
          "2017",
          6,
          15
        ]
      ]
    },
    "number": "arXiv:1609.04747",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "An overview of gradient descent optimization algorithms",
    "type": "article",
    "URL": "http://arxiv.org/abs/1609.04747"
  },
  {
    "id": "Cauchy1897",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          16
        ]
      ]
    },
    "citation-key": "Cauchy1897",
    "collection-title": "Cambridge Library Collection - Mathematics",
    "container-title": "Oeuvres complètes: Series 1",
    "DOI": "10.1017/CBO9780511702396.063",
    "editor": [
      {
        "family": "Cauchy",
        "given": "Augustin-Louis"
      }
    ],
    "ISBN": "978-1-108-00277-6",
    "issued": {
      "date-parts": [
        [
          "1897"
        ]
      ]
    },
    "page": "399–402",
    "publisher": "Cambridge University Press",
    "publisher-place": "Cambridge",
    "source": "Cambridge University Press",
    "title": "Méthode générale pour la résolution des systèmes d'équations simultanées",
    "type": "chapter",
    "URL": "https://www.cambridge.org/core/books/oeuvres-completes/analyse-mathematique-methodc-generale-pour-la-resolution-des-systemes-dequations-simultanees/74A15B57C79B65C956D558CFA8766693",
    "volume": "10"
  },
  {
    "id": "Robbins1951",
    "abstract": "Let $M(x)$ denote the expected value at level $x$ of the response to a certain experiment. $M(x)$ is assumed to be a monotone function of $x$ but is unknown to the experimenter, and it is desired to find the solution $x = \\theta$ of the equation $M(x) = \\alpha$, where $\\alpha$ is a given constant. We give a method for making successive experiments at levels $x_1,x_2,\\cdots$ in such a way that $x_n$ will tend to $\\theta$ in probability.",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          12
        ]
      ]
    },
    "author": [
      {
        "family": "Robbins",
        "given": "Herbert"
      },
      {
        "family": "Monro",
        "given": "Sutton"
      }
    ],
    "citation-key": "Robbins1951",
    "container-title": "The Annals of Mathematical Statistics",
    "DOI": "10.1214/aoms/1177729586",
    "ISSN": "0003-4851, 2168-8990",
    "issue": "3",
    "issued": {
      "date-parts": [
        [
          "1951",
          9
        ]
      ]
    },
    "page": "400–407",
    "publisher": "Institute of Mathematical Statistics",
    "source": "Project Euclid",
    "title": "A Stochastic Approximation Method",
    "type": "article-journal",
    "URL": "https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full",
    "volume": "22"
  },
  {
    "id": "Polyak1964",
    "abstract": "For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          16
        ]
      ]
    },
    "author": [
      {
        "family": "Polyak",
        "given": "B. T."
      }
    ],
    "citation-key": "Polyak1964",
    "container-title": "USSR Computational Mathematics and Mathematical Physics",
    "container-title-short": "USSR Computational Mathematics and Mathematical Physics",
    "DOI": "10.1016/0041-5553(64)90137-5",
    "ISSN": "0041-5553",
    "issue": "5",
    "issued": {
      "date-parts": [
        [
          "1964",
          1,
          1
        ]
      ]
    },
    "page": "1–17",
    "source": "ScienceDirect",
    "title": "Some methods of speeding up the convergence of iteration methods",
    "type": "article-journal",
    "URL": "https://www.sciencedirect.com/science/article/pii/0041555364901375",
    "volume": "4"
  },
  {
    "id": "Qian1999",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          13
        ]
      ]
    },
    "author": [
      {
        "family": "Qian",
        "given": "Ning"
      }
    ],
    "citation-key": "Qian1999",
    "container-title": "Neural Networks",
    "container-title-short": "Neural Networks",
    "DOI": "10.1016/S0893-6080(98)00116-6",
    "ISSN": "08936080",
    "issue": "1",
    "issued": {
      "date-parts": [
        [
          "1999",
          1
        ]
      ]
    },
    "language": "en",
    "page": "145–151",
    "source": "DOI.org (Crossref)",
    "title": "On the momentum term in gradient descent learning algorithms",
    "type": "article-journal",
    "URL": "https://linkinghub.elsevier.com/retrieve/pii/S0893608098001166",
    "volume": "12"
  },
  {
    "id": "Nesterov1983",
    "abstract": "Semantic Scholar extracted view of \"A method for solving the convex programming problem with convergence rate O(1/k^2)\" by Y. Nesterov",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          16
        ]
      ]
    },
    "author": [
      {
        "family": "Nesterov",
        "given": "Y."
      }
    ],
    "citation-key": "Nesterov1983",
    "container-title": "Proceedings of the USSR Academy of Sciences",
    "issued": {
      "date-parts": [
        [
          "1983"
        ]
      ]
    },
    "source": "Semantic Scholar",
    "title": "A method for solving the convex programming problem with convergence rate O(1/k^2)",
    "type": "article-journal",
    "URL": "https://www.semanticscholar.org/paper/A-method-for-solving-the-convex-programming-problem-Nesterov/8d3a318b62d2e970122da35b2a2e70a5d12cc16f"
  },
  {
    "id": "Koolen2017",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          19
        ]
      ]
    },
    "author": [
      {
        "family": "Koolen",
        "given": "Wouter M."
      }
    ],
    "citation-key": "Koolen2017",
    "issued": {
      "date-parts": [
        [
          "2017",
          1,
          21
        ]
      ]
    },
    "title": "The Advantage of Full AdaGrad",
    "type": "webpage",
    "URL": "https://blog.wouterkoolen.info/GrokkingAdaGrad/post.html"
  },
  {
    "id": "Duchi2011",
    "abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          16
        ]
      ]
    },
    "author": [
      {
        "family": "Duchi",
        "given": "John"
      },
      {
        "family": "Hazan",
        "given": "Elad"
      },
      {
        "family": "Singer",
        "given": "Yoram"
      }
    ],
    "citation-key": "Duchi2011",
    "container-title": "J. Mach. Learn. Res.",
    "ISSN": "1532-4435",
    "issued": {
      "date-parts": [
        [
          "2011",
          7,
          1
        ]
      ]
    },
    "page": "2121–2159",
    "source": "ACM Digital Library",
    "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
    "type": "article-journal",
    "URL": "https://dl.acm.org/doi/10.5555/1953048.2021068",
    "volume": "12"
  },
  {
    "id": "Hinton2012",
    "abstract": "Share your videos with friends, family and the world",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          16
        ]
      ]
    },
    "citation-key": "Hinton2012",
    "director": [
      {
        "family": "Hinton",
        "given": "Geoffrey"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "language": "en-GB",
    "title": "Neural Networks for Machine Learning",
    "type": "motion_picture"
  },
  {
    "id": "Zeiler2012",
    "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          16
        ]
      ]
    },
    "author": [
      {
        "family": "Zeiler",
        "given": "Matthew D."
      }
    ],
    "citation-key": "Zeiler2012",
    "DOI": "10.48550/ARXIV.1212.5701",
    "issued": {
      "date-parts": [
        [
          "2012"
        ]
      ]
    },
    "license": "arXiv.org perpetual, non-exclusive license",
    "publisher": "arXiv",
    "source": "DOI.org (Datacite)",
    "title": "ADADELTA: An Adaptive Learning Rate Method",
    "title-short": "ADADELTA",
    "type": "article",
    "URL": "https://arxiv.org/abs/1212.5701",
    "version": "1"
  },
  {
    "id": "Kingma2017",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          10
        ]
      ]
    },
    "author": [
      {
        "family": "Kingma",
        "given": "Diederik P."
      },
      {
        "family": "Ba",
        "given": "Jimmy"
      }
    ],
    "citation-key": "Kingma2017",
    "DOI": "10.48550/arXiv.1412.6980",
    "issued": {
      "date-parts": [
        [
          "2017",
          1,
          30
        ]
      ]
    },
    "number": "arXiv:1412.6980",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Adam: A Method for Stochastic Optimization",
    "title-short": "Adam",
    "type": "article",
    "URL": "http://arxiv.org/abs/1412.6980"
  },
  {
    "id": "Reddi2018",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          13
        ]
      ]
    },
    "author": [
      {
        "family": "Reddi",
        "given": "Sashank J."
      },
      {
        "family": "Kale",
        "given": "Satyen"
      },
      {
        "family": "Kumar",
        "given": "Surinder"
      }
    ],
    "citation-key": "Reddi2018",
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          "2018",
          2,
          15
        ]
      ]
    },
    "source": "Semantic Scholar",
    "title": "On the Convergence of Adam and Beyond",
    "type": "article-journal",
    "URL": "https://www.semanticscholar.org/paper/On-the-Convergence-of-Adam-and-Beyond-Reddi-Kale/c983653841b6987d9959318f074a595783838576"
  },
  {
    "id": "Loshchilov2019",
    "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW",
    "accessed": {
      "date-parts": [
        [
          "2026",
          2,
          16
        ]
      ]
    },
    "author": [
      {
        "family": "Loshchilov",
        "given": "Ilya"
      },
      {
        "family": "Hutter",
        "given": "Frank"
      }
    ],
    "citation-key": "Loshchilov2019",
    "DOI": "10.48550/arXiv.1711.05101",
    "issued": {
      "date-parts": [
        [
          "2019",
          1,
          4
        ]
      ]
    },
    "number": "arXiv:1711.05101",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Decoupled Weight Decay Regularization",
    "type": "article",
    "URL": "http://arxiv.org/abs/1711.05101"
  }
]
